{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Word embedding in Python Gensim\n",
    "\n",
    "#### based on: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:/Users/renswilderom/Documents/Machine learning'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/renswilderom/Documents/Dan Silver Projects/history of sociology/stm_prep.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'TEXT'] = df.loc[:, 'TEXT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split documents into sentences\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "df['sentences'] = df.apply(lambda row: tokenizer.tokenize(row['TEXT']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'sentences'] = df.loc[:, 'sentences'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(df.loc[1, 'sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "df['tokenized_sentences'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(df.loc[1, 'tokenized_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Slice data frame in periods\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df_temp[df_temp.YEAR < 1926] # 1895-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df_temp[(df_temp.YEAR > 1925) & (df_temp.YEAR < 1956)] # 1926-1955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = df_temp[(df_temp.YEAR > 1955) & (df_temp.YEAR < 1986)] # 1956-1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3934, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4 = df_temp[(df_temp.YEAR > 1985)] # 1986-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6119, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Train models\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renswilderom\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "model1 = Word2Vec(df1.tokenized_sentences, min_count=1) # 1895-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = Word2Vec(df2.tokenized_sentences, min_count=1) # 1926-1955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Word2Vec(df3.tokenized_sentences, min_count=1) # 1956-1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = Word2Vec(df4.tokenized_sentences, min_count=1) # 1986-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 10 most similar words vis-a-vis some words of interest\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**relation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8441445231437683),\n",
       " ('relations', 0.7866001129150391),\n",
       " ('opposition', 0.7335350513458252),\n",
       " ('function', 0.7322505712509155),\n",
       " ('response', 0.7263873219490051),\n",
       " ('antagonism', 0.7014156579971313),\n",
       " ('reference', 0.6968504190444946),\n",
       " ('distinction', 0.688345730304718),\n",
       " ('structure', 0.6882886290550232),\n",
       " ('conflict', 0.6839989423751831)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('relation') # 1895-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8417072296142578),\n",
       " ('similarity', 0.6826012134552002),\n",
       " ('resemblance', 0.6686841249465942),\n",
       " ('association', 0.6534318923950195),\n",
       " ('relations', 0.650056004524231),\n",
       " ('link', 0.6373822689056396),\n",
       " ('contrast', 0.6302788853645325),\n",
       " ('interrelationship', 0.6288437843322754),\n",
       " ('interrelation', 0.6260203123092651),\n",
       " ('interrelationships', 0.6189782619476318)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('relation') # 1926-1955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8869860172271729),\n",
       " ('link', 0.7533186674118042),\n",
       " ('relationships', 0.7214893102645874),\n",
       " ('connection', 0.7156462073326111),\n",
       " ('association', 0.7111459970474243),\n",
       " ('linkage', 0.7057282328605652),\n",
       " ('tie', 0.6991400718688965),\n",
       " ('relations', 0.6889328956604004),\n",
       " ('nexus', 0.688151478767395),\n",
       " ('socially-defined', 0.6826616525650024)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('relation') # 1956-1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.852817952632904),\n",
       " ('connection', 0.7491021156311035),\n",
       " ('link', 0.6950960159301758),\n",
       " ('linkage', 0.6906623840332031),\n",
       " ('interplay', 0.6799286603927612),\n",
       " ('relations', 0.6672086119651794),\n",
       " ('Netherlands160', 0.6413803696632385),\n",
       " ('links', 0.637802243232727),\n",
       " ('relationships', 0.6342426538467407),\n",
       " ('interrelationship', 0.6274843215942383)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('relation') # 1986-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cultural**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('racial', 0.8538060188293457),\n",
       " ('functional', 0.823914110660553),\n",
       " ('psychic', 0.8227604031562805),\n",
       " ('psychical', 0.810051679611206),\n",
       " ('genetic', 0.8015466332435608),\n",
       " ('chemical', 0.8008708953857422),\n",
       " ('geographical', 0.7990705966949463),\n",
       " ('physiological', 0.7939574718475342),\n",
       " ('organic', 0.7920008301734924),\n",
       " ('internal', 0.7879409790039062)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('cultural') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('culture', 0.8146034479141235),\n",
       " ('structural', 0.791475772857666),\n",
       " ('societal', 0.7879284024238586),\n",
       " ('institutional', 0.767919659614563),\n",
       " ('functional', 0.7537322044372559),\n",
       " ('biological', 0.7528942823410034),\n",
       " ('social', 0.7403719425201416),\n",
       " ('dynamic', 0.7374778389930725),\n",
       " ('racial', 0.7342513799667358),\n",
       " ('linguistic', 0.7221019268035889)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ideological', 0.7420690059661865),\n",
       " ('linguistic', 0.7322743535041809),\n",
       " ('distinctive', 0.7210464477539062),\n",
       " ('cognitive', 0.7178052067756653),\n",
       " ('subcultural', 0.7093836069107056),\n",
       " ('institutional', 0.7090656757354736),\n",
       " ('religious', 0.6968305110931396),\n",
       " ('culture', 0.694295346736908),\n",
       " ('secular', 0.6935019493103027),\n",
       " ('sociocultural', 0.6796746253967285)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('symbolic', 0.7528668642044067),\n",
       " ('discursive', 0.7232345342636108),\n",
       " ('institutional', 0.6804810762405396),\n",
       " ('linguistic', 0.6664934158325195),\n",
       " ('artistic', 0.655575156211853),\n",
       " ('ideological', 0.6555702686309814),\n",
       " ('culture', 0.6552258729934692),\n",
       " ('political', 0.6521793603897095),\n",
       " ('structural', 0.6398521065711975),\n",
       " ('religious', 0.6380373239517212)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Dan: \n",
    "#### 'causal', 'causality', ‘objective’, ‘objectivity’, ‘subjective’, ‘subjectivity’, ‘schema’, ‘Simmel’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**causal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conditioning', 0.7963470220565796),\n",
       " ('functional', 0.7872750759124756),\n",
       " ('reciprocal', 0.7838644981384277),\n",
       " ('physiological', 0.7837982773780823),\n",
       " ('psychic', 0.7819134593009949),\n",
       " ('societary', 0.7752439975738525),\n",
       " ('quantitative', 0.774134635925293),\n",
       " ('external', 0.7653709650039673),\n",
       " ('environmental', 0.7648234367370605),\n",
       " ('cultural', 0.7598253488540649)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('functional', 0.7950720191001892),\n",
       " ('situational', 0.7687092423439026),\n",
       " ('qualitative', 0.737224817276001),\n",
       " ('structural', 0.7348059415817261),\n",
       " ('genetic', 0.7337712049484253),\n",
       " ('logical', 0.7255640625953674),\n",
       " ('behavioral', 0.7108860611915588),\n",
       " ('temporal', 0.7089613080024719),\n",
       " ('interpersonal', 0.706243097782135),\n",
       " ('societal', 0.7052133083343506)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reciprocal', 0.7385190725326538),\n",
       " ('logical', 0.7241897583007812),\n",
       " ('structural', 0.7147865295410156),\n",
       " ('simultaneous', 0.6898951530456543),\n",
       " ('linear', 0.6840468049049377),\n",
       " ('sequential', 0.682436466217041),\n",
       " ('causality', 0.674077570438385),\n",
       " ('temporal', 0.6668140888214111),\n",
       " ('developmental', 0.6650990843772888),\n",
       " ('contextual', 0.6519103050231934)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('generative', 0.7024340033531189),\n",
       " ('causality', 0.7003788948059082),\n",
       " ('contextual', 0.69304358959198),\n",
       " ('motivational', 0.6824032664299011),\n",
       " ('logical', 0.6732432842254639),\n",
       " ('structural', 0.6722752451896667),\n",
       " ('underlying', 0.6615617275238037),\n",
       " ('situational', 0.661487877368927),\n",
       " ('functional', 0.6401398181915283),\n",
       " ('systemic', 0.6374400854110718)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('causal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**causality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conditions—with', 0.6643602252006531),\n",
       " ('wh', 0.6520610451698303),\n",
       " ('acids', 0.6250447034835815),\n",
       " ('™Ibid', 0.6136768460273743),\n",
       " ('optimism', 0.6132846474647522),\n",
       " ('tension', 0.613148033618927),\n",
       " ('mand', 0.6128140687942505),\n",
       " ('differences—which', 0.6061928272247314),\n",
       " ('Grave', 0.5998084545135498),\n",
       " ('suffixes', 0.5987680554389954)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('causality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dialectic', 0.7443912029266357),\n",
       " ('relativity', 0.739704966545105),\n",
       " ('gestalt', 0.733802855014801),\n",
       " ('schema', 0.7336994409561157),\n",
       " ('immanent', 0.7328548431396484),\n",
       " ('mechanistic', 0.7053650617599487),\n",
       " ('particularistic', 0.7034801244735718),\n",
       " ('role-taking', 0.6971359252929688),\n",
       " ('transcendental', 0.6966195106506348),\n",
       " ('means-end', 0.6915581226348877)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('causality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('causation', 0.7593010663986206),\n",
       " ('causal', 0.6740775108337402),\n",
       " ('reciprocity', 0.6695437431335449),\n",
       " ('linkage', 0.6636263132095337),\n",
       " ('symmetry', 0.661018967628479),\n",
       " ('teleological', 0.6564818620681763),\n",
       " ('consistency', 0.6491689682006836),\n",
       " ('dissonance', 0.6416134834289551),\n",
       " ('contagion', 0.6347087621688843),\n",
       " ('closure', 0.6303196549415588)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('causality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('causation', 0.8278628587722778),\n",
       " ('contingency', 0.7452189922332764),\n",
       " ('reification', 0.7374314665794373),\n",
       " ('causal', 0.7003788948059082),\n",
       " ('distortion', 0.6949384212493896),\n",
       " ('inference', 0.6869561076164246),\n",
       " ('generalization', 0.6716681122779846),\n",
       " ('ontology', 0.6602618098258972),\n",
       " ('teleological', 0.6595219969749451),\n",
       " ('reflexivity', 0.6540914177894592)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('causality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**objective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('psychological', 0.8542784452438354),\n",
       " ('biological', 0.8342928886413574),\n",
       " ('abstract', 0.8336150050163269),\n",
       " ('psychic', 0.8332881927490234),\n",
       " ('organic', 0.8310183882713318),\n",
       " ('subjective', 0.8243480324745178),\n",
       " ('concrete', 0.8170121312141418),\n",
       " ('logical', 0.8091793060302734),\n",
       " ('external', 0.8055845499038696),\n",
       " ('positive', 0.8022143244743347)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empirical', 0.7851575613021851),\n",
       " ('subjective', 0.7695214152336121),\n",
       " ('meaningful', 0.7631130814552307),\n",
       " ('quantitative', 0.7607792615890503),\n",
       " ('analytical', 0.7562923431396484),\n",
       " ('abstract', 0.7516562342643738),\n",
       " ('operational', 0.7502760887145996),\n",
       " ('measurement', 0.7403283715248108),\n",
       " ('abstraction', 0.7243492603302002),\n",
       " ('evaluation', 0.7231173515319824)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subjective', 0.6697182059288025),\n",
       " ('essential', 0.6574784517288208),\n",
       " ('ontological', 0.6436105966567993),\n",
       " ('abstract', 0.6423295736312866),\n",
       " ('motive', 0.640049397945404),\n",
       " ('object', 0.6359859108924866),\n",
       " ('cognitive', 0.6349653005599976),\n",
       " ('underlying', 0.6340020895004272),\n",
       " ('intrinsic', 0.6331543922424316),\n",
       " ('ultimate', 0.6249767541885376)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('immanent', 0.7301717400550842),\n",
       " ('intrinsic', 0.7074230909347534),\n",
       " ('subjective', 0.6827552318572998),\n",
       " ('ontological', 0.677768349647522),\n",
       " ('underlying', 0.6422537565231323),\n",
       " ('existential', 0.6377660632133484),\n",
       " ('irreducible', 0.6234538555145264),\n",
       " ('illusory', 0.6212280988693237),\n",
       " ('unobservable', 0.6198253631591797),\n",
       " ('meaning', 0.6139387488365173)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**objectivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potency', 0.7457101941108704),\n",
       " ('group-life', 0.7335221171379089),\n",
       " ('expansive', 0.730195164680481),\n",
       " ('coherence', 0.7163501977920532),\n",
       " ('speculation', 0.7050509452819824),\n",
       " ('enlightenment', 0.7042633295059204),\n",
       " ('altruism', 0.7038308382034302),\n",
       " ('multiplicity', 0.7023306488990784),\n",
       " ('simplicity', 0.6998115181922913),\n",
       " ('protoplasm', 0.6968473196029663)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('precision', 0.7891201972961426),\n",
       " ('clarification', 0.759067177772522),\n",
       " ('clarity', 0.7479539513587952),\n",
       " ('logic', 0.7403373718261719),\n",
       " ('verification', 0.7258188724517822),\n",
       " ('rationality', 0.722417950630188),\n",
       " ('motivation', 0.7221505641937256),\n",
       " ('insight', 0.7175171375274658),\n",
       " ('curiosity', 0.7163212299346924),\n",
       " ('speculation', 0.7132241129875183)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('truth', 0.7581232190132141),\n",
       " ('canons', 0.7061920762062073),\n",
       " ('intuitive', 0.6949276924133301),\n",
       " ('universality', 0.6877827048301697),\n",
       " ('clarity', 0.6828929781913757),\n",
       " ('generality', 0.6786223649978638),\n",
       " ('coherence', 0.6779695749282837),\n",
       " ('ignorance', 0.6775922775268555),\n",
       " ('judgment', 0.677091658115387),\n",
       " ('reductionism', 0.6743451356887817)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universality', 0.7826473712921143),\n",
       " ('truth', 0.7760058641433716),\n",
       " ('transcendental', 0.7242398262023926),\n",
       " ('metaphysical', 0.7142008543014526),\n",
       " ('realism', 0.7102657556533813),\n",
       " ('irrationality', 0.7091241478919983),\n",
       " ('neutrality', 0.7074251174926758),\n",
       " ('disinterestedness', 0.7066991329193115),\n",
       " ('authenticity', 0.7052381038665771),\n",
       " ('relativism', 0.7046592235565186)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subjective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('psychic', 0.8662709593772888),\n",
       " ('biological', 0.8498037457466125),\n",
       " ('psychical', 0.8344156742095947),\n",
       " ('functional', 0.8304904699325562),\n",
       " ('objective', 0.8243480324745178),\n",
       " ('physiological', 0.808037281036377),\n",
       " ('psychological', 0.8028580546379089),\n",
       " ('genetic', 0.7957460284233093),\n",
       " ('negative', 0.7794422507286072),\n",
       " ('static', 0.7757461071014404)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('symbolic', 0.7775448560714722),\n",
       " ('objective', 0.7695214152336121),\n",
       " ('verbal', 0.752875804901123),\n",
       " ('particularistic', 0.7525017261505127),\n",
       " ('psychological', 0.7500637173652649),\n",
       " ('situational', 0.7421325445175171),\n",
       " ('rational', 0.7366851568222046),\n",
       " ('biological', 0.7233442664146423),\n",
       " ('intrinsic', 0.7225503325462341),\n",
       " ('psychic', 0.7221026420593262)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cognitive', 0.7481263875961304),\n",
       " ('intrinsic', 0.7241697311401367),\n",
       " ('behavioral', 0.7099941372871399),\n",
       " ('affective', 0.7076915502548218),\n",
       " ('verbal', 0.6921893954277039),\n",
       " ('normative', 0.6806974411010742),\n",
       " ('fairness', 0.6782318353652954),\n",
       " ('psychological', 0.6723742485046387),\n",
       " ('objective', 0.6697181463241577),\n",
       " ('situational', 0.6695312857627869)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('situational', 0.6981050968170166),\n",
       " ('affective', 0.6927733421325684),\n",
       " ('cognitive', 0.6842795610427856),\n",
       " ('objective', 0.6827552318572998),\n",
       " ('unconscious', 0.6807843446731567),\n",
       " ('performative', 0.6802817583084106),\n",
       " ('intersubjective', 0.6655670404434204),\n",
       " ('normative', 0.660075843334198),\n",
       " ('non-rational', 0.658427357673645),\n",
       " ('expressive', 0.6557738184928894)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subjectivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('modesty', 0.8255383372306824),\n",
       " ('Iowa.1', 0.8158161640167236),\n",
       " ('Measurement', 0.8146536946296692),\n",
       " ('Countess', 0.8115413188934326),\n",
       " ('jingoism', 0.8095638751983643),\n",
       " ('institutionalism', 0.8089417815208435),\n",
       " ('serviceability', 0.8073192834854126),\n",
       " ('uajpqqQ', 0.8045661449432373),\n",
       " ('sealed', 0.8032026290893555),\n",
       " ('Orestes', 0.802842378616333)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('subjectivity')\n",
    "# the model picks up numbers, suggesting that subjectivity was not really used frequently in this early period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 0.7131853699684143),\n",
       " ('correctness', 0.7116696834564209),\n",
       " ('specificity', 0.7049423456192017),\n",
       " ('generality', 0.6749253273010254),\n",
       " ('inapplicability', 0.6672554612159729),\n",
       " ('psychopathy', 0.6617553234100342),\n",
       " ('vagueness', 0.6614216566085815),\n",
       " ('soundness', 0.6539335250854492),\n",
       " ('distortion', 0.6529852747917175),\n",
       " ('likeness', 0.6483901739120483)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reification', 0.7334219217300415),\n",
       " ('intersubjectivity', 0.7204539179801941),\n",
       " ('superego', 0.6862199306488037),\n",
       " ('sociality', 0.6741387844085693),\n",
       " ('contemplation', 0.6740444302558899),\n",
       " ('irrationality', 0.6727896332740784),\n",
       " ('transcendent', 0.6678208112716675),\n",
       " ('femininity', 0.6636308431625366),\n",
       " ('sacredness', 0.6585168838500977),\n",
       " ('intersubjective', 0.658326268196106)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sociality', 0.789771556854248),\n",
       " ('selfhood', 0.7740150690078735),\n",
       " ('consciousness', 0.7684578895568848),\n",
       " ('individuality', 0.7628229856491089),\n",
       " ('self', 0.7525850534439087),\n",
       " ('reflexivity', 0.7482286691665649),\n",
       " ('historicity', 0.7472134828567505),\n",
       " ('intersubjectivity', 0.740371823310852),\n",
       " ('praxis', 0.7358789443969727),\n",
       " ('embodiment', 0.7291556596755981)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('subjectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terminology', 0.7195020914077759),\n",
       " ('keynote', 0.712675154209137),\n",
       " ('mood', 0.703575611114502),\n",
       " ('scene', 0.7011400461196899),\n",
       " ('background', 0.6937474012374878),\n",
       " ('translation', 0.6920735836029053),\n",
       " ('exposition', 0.6904094219207764),\n",
       " ('formula', 0.6882789134979248),\n",
       " ('creed', 0.6820672154426575),\n",
       " ('nationalism', 0.678968071937561)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('restatement', 0.7641053795814514),\n",
       " ('sequence-pattern', 0.747427225112915),\n",
       " ('means-end', 0.7419079542160034),\n",
       " ('synthesis', 0.7343186140060425),\n",
       " ('causality', 0.7336994409561157),\n",
       " ('paradigm', 0.7165526151657104),\n",
       " ('relativity', 0.710946798324585),\n",
       " ('typology', 0.6994298696517944),\n",
       " ('formulation', 0.68601393699646),\n",
       " ('scaling', 0.6852098703384399)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('typology', 0.7986904382705688),\n",
       " ('scheme', 0.7977442145347595),\n",
       " ('formulation', 0.7934887409210205),\n",
       " ('epistemology', 0.7823007106781006),\n",
       " ('conceptualization', 0.7634795308113098),\n",
       " ('paradigm', 0.756039023399353),\n",
       " ('logic', 0.7507690191268921),\n",
       " ('approach', 0.749588131904602),\n",
       " ('formulations', 0.7438548803329468),\n",
       " ('methodology', 0.7434911727905273)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scheme', 0.8380883932113647),\n",
       " ('framework', 0.7660828828811646),\n",
       " ('typology', 0.7586386203765869),\n",
       " ('model', 0.7298818826675415),\n",
       " ('conceptualization', 0.7246353626251221),\n",
       " ('formulation', 0.7202170491218567),\n",
       " ('grammar', 0.7196730375289917),\n",
       " ('concept', 0.7125412821769714),\n",
       " ('definition', 0.7045414447784424),\n",
       " ('conception', 0.7020230293273926)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simmel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Small', 0.8609300851821899),\n",
       " ('Durkheim', 0.8544952273368835),\n",
       " ('Wundt', 0.8492485284805298),\n",
       " ('Ellwood', 0.8444584012031555),\n",
       " ('Ross', 0.8423856496810913),\n",
       " ('Baldwin', 0.8358311653137207),\n",
       " ('Sumner', 0.8232913017272949),\n",
       " ('Dewey', 0.8184419870376587),\n",
       " ('Morgan', 0.8159871101379395),\n",
       " ('McDougall', 0.8030745983123779)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Durkheim', 0.8674623370170593),\n",
       " ('Merton', 0.8641049861907959),\n",
       " ('Karl', 0.8611164093017578),\n",
       " ('Becker', 0.8603239059448242),\n",
       " ('Malinowski', 0.8490426540374756),\n",
       " ('Herbert', 0.8468589186668396),\n",
       " ('Dewey', 0.8459092378616333),\n",
       " ('William', 0.8436660766601562),\n",
       " ('Sumner', 0.8427450060844421),\n",
       " ('Charles', 0.8427026271820068)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spencer', 0.865730881690979),\n",
       " ('Weber', 0.8276360034942627),\n",
       " ('Mannheim', 0.8234241008758545),\n",
       " ('Mead', 0.8222999572753906),\n",
       " ('Comte', 0.8141987919807434),\n",
       " ('Freud', 0.7995558977127075),\n",
       " ('Malinowski', 0.7956452369689941),\n",
       " ('Durkheim', 0.792294979095459),\n",
       " ('Saint-Simon', 0.7857382297515869),\n",
       " ('Pareto', 0.7819395065307617)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Durkheim', 0.8434534668922424),\n",
       " ('Weber', 0.8308014869689941),\n",
       " ('Elias', 0.8087160587310791),\n",
       " ('Castoriadis', 0.7933200597763062),\n",
       " ('Hegel', 0.7855691909790039),\n",
       " ('Mannheim', 0.78470778465271),\n",
       " ('Whitehead', 0.7726262211799622),\n",
       " ('Bergson', 0.7698306441307068),\n",
       " ('Heidegger', 0.7675184607505798),\n",
       " ('Freud', 0.7644785642623901)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## End of script\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## notes and draft\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TSNE vis https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "* TSNE vis http://lvdmaaten.github.io/tsne/\n",
    "* Second Tensorboard visualization demo: https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8\n",
    "* Word to Vec explained: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* Using .loc: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
    "* Gensim Word2Vec: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "* SNA in Jupyter Notebook: http://bl.ocks.org/brinrosenthal/raw/fd7d7277ce74c2b762d3a4d66326215c/\n",
    "* Same but than the blog version: http://compbio.ucsd.edu/bringing-interactivity-network-visualization-jupyter-notebooks-visjs2jupyter/\n",
    "* Nice word embedding blog\n",
    "* Presentation Manning: https://nlp.stanford.edu/manning/talks/Simons-Institute-Manning-2017.pdf\n",
    "* what about stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After the model is trained, it is accessible via the “wv” attribute. This is the actual word vector model in which queries can be made.\n",
    "\n",
    "# For example, you can print the learned vocabulary of tokens (words) as follows:\n",
    "\n",
    "words = list(model_test.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can review the embedded vector for a specific token as follows:\n",
    "\n",
    "print(model_test['system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test.wv.most_similar(positive=['system', 'social'], negative=['socialism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test.wv.doesnt_match(\"system connected developed science\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test.wv.similarity('causal', 'relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When getting started, you can save the learned model in ASCII format and review the contents.\n",
    "\n",
    "# You can do this by setting binary=False when calling the save_word2vec_format() function, for example:\n",
    "\n",
    "# model_test.wv.save_word2vec_format('model_test.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The saved model can then be loaded again by calling the Word2Vec.load() function. For example:\n",
    "# model = Word2Vec.load('model_test.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Example codes from tutorial\n",
    "#### https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# specify the main corpus path. This will be used throughout the script\n",
    "MACHINE_LEARNING = \"C:/Users/renswilderom/Documents/Machine learning\"\n",
    "\n",
    "#Specify working directory\n",
    "os.chdir(MACHINE_LEARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['normal', 'child'], negative=['school'], topn=1)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
