{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Word embedding in Python Gensim\n",
    "\n",
    "#### based on: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:/Users/renswilderom/Documents/Machine learning'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/renswilderom/Documents/Dan Silver Projects/history of sociology/stm_prep.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'TEXT'] = df.loc[:, 'TEXT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split documents into sentences\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "df['sentences'] = df.apply(lambda row: tokenizer.tokenize(row['TEXT']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'sentences'] = df.loc[:, 'sentences'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(df.loc[1, 'sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "df['tokenized_sentences'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(df.loc[1, 'tokenized_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Slice data frame in periods\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df_temp[df_temp.YEAR < 1926] # 1895-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df_temp[(df_temp.YEAR > 1925) & (df_temp.YEAR < 1956)] # 1926-1955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = df_temp[(df_temp.YEAR > 1955) & (df_temp.YEAR < 1986)] # 1956-1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3934, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4 = df_temp[(df_temp.YEAR > 1985)] # 1986-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6119, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Train models\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renswilderom\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "model1 = Word2Vec(df1.tokenized_sentences, min_count=1) # 1895-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = Word2Vec(df2.tokenized_sentences, min_count=1) # 1926-1955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Word2Vec(df3.tokenized_sentences, min_count=1) # 1956-1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = Word2Vec(df4.tokenized_sentences, min_count=1) # 1986-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 10 most similar words vis-a-vis some words of interest\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**relation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8659447431564331),\n",
       " ('relations', 0.7774339914321899),\n",
       " ('response', 0.7536336183547974),\n",
       " ('function', 0.741924524307251),\n",
       " ('opposition', 0.722886323928833),\n",
       " ('adjustment', 0.7080439925193787),\n",
       " ('reference', 0.7015237808227539),\n",
       " ('distinction', 0.6922434568405151),\n",
       " ('status', 0.6879868507385254),\n",
       " ('tendency', 0.685497522354126)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('relation') # 1895-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8475290536880493),\n",
       " ('resemblance', 0.7025606632232666),\n",
       " ('relations', 0.681898832321167),\n",
       " ('relationships', 0.6548746824264526),\n",
       " ('association', 0.6409438252449036),\n",
       " ('similarity', 0.6373202204704285),\n",
       " ('interrelations', 0.6372702717781067),\n",
       " ('nexus', 0.6235227584838867),\n",
       " ('respect', 0.6112664937973022),\n",
       " ('contrast', 0.6106635332107544)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('relation') # 1926-1955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8870164155960083),\n",
       " ('link', 0.748445987701416),\n",
       " ('relationships', 0.7303850650787354),\n",
       " ('connection', 0.718569278717041),\n",
       " ('association', 0.7144267559051514),\n",
       " ('linkage', 0.712634801864624),\n",
       " ('tie', 0.7036232948303223),\n",
       " ('relations', 0.6887329816818237),\n",
       " ('nexus', 0.6794794201850891),\n",
       " ('links', 0.6668810248374939)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('relation') # 1956-1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relationship', 0.8372057676315308),\n",
       " ('connection', 0.7463864088058472),\n",
       " ('link', 0.6992675065994263),\n",
       " ('linkage', 0.6870735883712769),\n",
       " ('interplay', 0.676097571849823),\n",
       " ('relations', 0.6587059497833252),\n",
       " ('interrelations', 0.6364309787750244),\n",
       " ('relationships', 0.6203354597091675),\n",
       " ('polarity', 0.6110794544219971),\n",
       " ('difference', 0.6108248829841614)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('relation') # 1986-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cultural**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('racial', 0.839154839515686),\n",
       " ('psychical', 0.8305606245994568),\n",
       " ('functional', 0.8304929733276367),\n",
       " ('psychic', 0.8282445669174194),\n",
       " ('chemical', 0.8225065469741821),\n",
       " ('geographical', 0.8072457909584045),\n",
       " ('secondary', 0.8065112829208374),\n",
       " ('societary', 0.8044930696487427),\n",
       " ('physiological', 0.7992188930511475),\n",
       " ('internal', 0.7977006435394287)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('culture', 0.8169986009597778),\n",
       " ('structural', 0.7952417135238647),\n",
       " ('biological', 0.7710198760032654),\n",
       " ('societal', 0.7623051404953003),\n",
       " ('functional', 0.7543084621429443),\n",
       " ('institutional', 0.7470966577529907),\n",
       " ('social', 0.7324671149253845),\n",
       " ('psychological', 0.7257303595542908),\n",
       " ('psychic', 0.717496395111084),\n",
       " ('dynamic', 0.7122875452041626)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linguistic', 0.7234350442886353),\n",
       " ('ideological', 0.7230170965194702),\n",
       " ('cognitive', 0.7189174890518188),\n",
       " ('institutional', 0.7158393859863281),\n",
       " ('secular', 0.7142602205276489),\n",
       " ('subcultural', 0.7104274034500122),\n",
       " ('distinctive', 0.7072334289550781),\n",
       " ('religious', 0.7016386985778809),\n",
       " ('behavioral', 0.688870370388031),\n",
       " ('normative', 0.6853549480438232)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('symbolic', 0.7510474920272827),\n",
       " ('linguistic', 0.6993869543075562),\n",
       " ('discursive', 0.6973252296447754),\n",
       " ('artistic', 0.6774075627326965),\n",
       " ('institutional', 0.6765850186347961),\n",
       " ('ideological', 0.6753431558609009),\n",
       " ('sociocultural', 0.6697101593017578),\n",
       " ('political', 0.664158821105957),\n",
       " ('ideational', 0.6546635627746582),\n",
       " ('aesthetic', 0.6495617032051086)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('cultural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Dan: \n",
    "#### ‘objective’, ‘objectivity’, ‘subjective’, ‘subjectivity’, ‘schema’, ‘Simmel’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**objective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('psychological', 0.8524646162986755),\n",
       " ('abstract', 0.8492943048477173),\n",
       " ('subjective', 0.829910159111023),\n",
       " ('positive', 0.8279579877853394),\n",
       " ('logical', 0.8264224529266357),\n",
       " ('ultimate', 0.823868989944458),\n",
       " ('organic', 0.8155571222305298),\n",
       " ('biological', 0.8127901554107666),\n",
       " ('psychic', 0.803335964679718),\n",
       " ('concrete', 0.8004611134529114)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('operational', 0.7728350758552551),\n",
       " ('meaningful', 0.7698444128036499),\n",
       " ('abstract', 0.7688069939613342),\n",
       " ('subjective', 0.7643225789070129),\n",
       " ('empirical', 0.7606490850448608),\n",
       " ('quantitative', 0.7583636045455933),\n",
       " ('measurement', 0.7483901977539062),\n",
       " ('abstraction', 0.7343795299530029),\n",
       " ('analytical', 0.7333526611328125),\n",
       " ('evaluation', 0.7240404486656189)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('object', 0.67438805103302),\n",
       " ('operational', 0.6578528881072998),\n",
       " ('essential', 0.654657244682312),\n",
       " ('cognitive', 0.6544842720031738),\n",
       " ('subjective', 0.6526729464530945),\n",
       " ('aggregation', 0.6505426168441772),\n",
       " ('motive', 0.6469446420669556),\n",
       " ('underlying', 0.6412361264228821),\n",
       " ('meaning', 0.6386818885803223),\n",
       " ('intrinsic', 0.6365792751312256)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intrinsic', 0.7122223377227783),\n",
       " ('immanent', 0.6869300603866577),\n",
       " ('subjective', 0.6860836744308472),\n",
       " ('ontological', 0.6432620286941528),\n",
       " ('underlying', 0.6426305174827576),\n",
       " ('ultimate', 0.6318342685699463),\n",
       " ('meaning', 0.6300592422485352),\n",
       " ('actual', 0.6297328472137451),\n",
       " ('existential', 0.6245933771133423),\n",
       " ('objectivity', 0.6208010315895081)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('objective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**objectivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coherence', 0.785571277141571),\n",
       " ('generality', 0.7423686981201172),\n",
       " ('contradiction', 0.7417454123497009),\n",
       " ('methodology', 0.7412897348403931),\n",
       " ('retreat', 0.7376754879951477),\n",
       " ('design', 0.7308334112167358),\n",
       " ('substratum', 0.7266759872436523),\n",
       " ('background', 0.722693145275116),\n",
       " ('humanitarianism', 0.7202526330947876),\n",
       " ('novelty', 0.7150938510894775)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clarity', 0.77120041847229),\n",
       " ('comprehension', 0.7646514177322388),\n",
       " ('motivation', 0.7588655352592468),\n",
       " ('speculation', 0.7501348853111267),\n",
       " ('clarification', 0.7429815530776978),\n",
       " ('truth', 0.7427546977996826),\n",
       " ('logic', 0.7378693222999573),\n",
       " ('perception', 0.7306792736053467),\n",
       " ('imagination', 0.727269172668457),\n",
       " ('reasoning', 0.7232944965362549)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universality', 0.7333980202674866),\n",
       " ('truth', 0.7082447409629822),\n",
       " ('relativism', 0.7047163844108582),\n",
       " ('ethical', 0.704587459564209),\n",
       " ('clarity', 0.7023354172706604),\n",
       " ('canons', 0.6802035570144653),\n",
       " ('sophistication', 0.675750195980072),\n",
       " ('formalism', 0.6751270294189453),\n",
       " ('neutrality', 0.6708697080612183),\n",
       " ('rationality', 0.6708270907402039)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('truth', 0.7801140546798706),\n",
       " ('universality', 0.7782642841339111),\n",
       " ('neutrality', 0.7599254250526428),\n",
       " ('irrationality', 0.7353039979934692),\n",
       " ('judgement', 0.7230526804924011),\n",
       " ('originality', 0.7225257754325867),\n",
       " ('transcendental', 0.7116068601608276),\n",
       " ('rationality', 0.7036386728286743),\n",
       " ('authenticity', 0.6976580023765564),\n",
       " ('metaphysical', 0.6971846222877502)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('objectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subjective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('psychic', 0.8744606971740723),\n",
       " ('psychical', 0.8654098510742188),\n",
       " ('biological', 0.8560588359832764),\n",
       " ('physiological', 0.8432759046554565),\n",
       " ('objective', 0.829910159111023),\n",
       " ('psychological', 0.821489155292511),\n",
       " ('functional', 0.8200857639312744),\n",
       " ('instinctive', 0.8038197159767151),\n",
       " ('genetic', 0.795669674873352),\n",
       " ('negative', 0.7941077351570129)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('verbal', 0.7737252712249756),\n",
       " ('conventional', 0.7685337066650391),\n",
       " ('objective', 0.7643226385116577),\n",
       " ('symbolic', 0.7632737159729004),\n",
       " ('evaluative', 0.7547131776809692),\n",
       " ('behavioral', 0.7420533299446106),\n",
       " ('particularistic', 0.7353549599647522),\n",
       " ('psychological', 0.7318572998046875),\n",
       " ('implicit', 0.7298088073730469),\n",
       " ('situational', 0.7247463464736938)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cognitive', 0.7446319460868835),\n",
       " ('intrinsic', 0.6979210376739502),\n",
       " ('affective', 0.6902922987937927),\n",
       " ('verbal', 0.6819781064987183),\n",
       " ('behavioral', 0.6807016134262085),\n",
       " ('personality', 0.6753392219543457),\n",
       " ('psychological', 0.6708508729934692),\n",
       " ('objective', 0.6526729464530945),\n",
       " ('normative', 0.6408082246780396),\n",
       " ('symbolic', 0.6322594881057739)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('objective', 0.6860836744308472),\n",
       " ('situational', 0.6760436296463013),\n",
       " ('corporeal', 0.6721557974815369),\n",
       " ('affective', 0.671944797039032),\n",
       " ('dispositional', 0.6658008694648743),\n",
       " ('physiological', 0.6584948301315308),\n",
       " ('expressive', 0.6532297134399414),\n",
       " ('behavioral', 0.6517412066459656),\n",
       " ('cognitive', 0.6468936204910278),\n",
       " ('intentional', 0.6468185186386108)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('subjective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subjectivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'Relief\", 0.7211368083953857),\n",
       " (\"'CHRISTIANITY\", 0.7149657011032104),\n",
       " ('suddenness', 0.7115455269813538),\n",
       " ('assertive', 0.7082674503326416),\n",
       " ('Sébastien', 0.7077919840812683),\n",
       " ('Faure', 0.7071225643157959),\n",
       " ('modesty', 0.705947995185852),\n",
       " ('waterworks.', 0.7038592100143433),\n",
       " ('™Ibid', 0.7032727599143982),\n",
       " ('machineries', 0.7029246091842651)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('subjectivity')\n",
    "# the model picks up numbers, suggesting that subjectivity was not really used frequently in this early period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('correctness', 0.7589266300201416),\n",
       " ('fruitfulness', 0.7455395460128784),\n",
       " ('verbalization', 0.7403524518013),\n",
       " ('universality', 0.7308475375175476),\n",
       " ('soundness', 0.7305565476417542),\n",
       " ('specificity', 0.7224956750869751),\n",
       " ('uniqueness', 0.7215976715087891),\n",
       " ('rationality', 0.7176700830459595),\n",
       " ('work.4', 0.7143925428390503),\n",
       " ('constancy', 0.711570143699646)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sacredness', 0.7629703879356384),\n",
       " ('intersubjectivity', 0.7543718218803406),\n",
       " ('reification', 0.7342047691345215),\n",
       " ('sociality', 0.734102725982666),\n",
       " ('praxis', 0.7116429805755615),\n",
       " ('superego', 0.7090240716934204),\n",
       " ('irrationality', 0.7051604986190796),\n",
       " ('supernatural', 0.6933643817901611),\n",
       " ('immortality', 0.6916900277137756),\n",
       " ('hedonism', 0.6879349946975708)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consciousness', 0.7812148332595825),\n",
       " ('self', 0.7622339725494385),\n",
       " ('intentionality', 0.7554757595062256),\n",
       " ('selfhood', 0.7524404525756836),\n",
       " ('sociality', 0.7517572641372681),\n",
       " ('reflexivity', 0.7450586557388306),\n",
       " ('individuality', 0.7324085235595703),\n",
       " ('agency', 0.727793276309967),\n",
       " ('cognition', 0.7256108522415161),\n",
       " ('intersubjectivity', 0.7250075936317444)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outcome', 0.7596882581710815),\n",
       " ('crisis', 0.7538934946060181),\n",
       " ('thesis', 0.7532879114151001),\n",
       " ('appearance', 0.749157190322876),\n",
       " ('mood', 0.7476211786270142),\n",
       " ('keynote', 0.7425433397293091),\n",
       " ('hypothesis', 0.738142728805542),\n",
       " ('transformation', 0.7305818200111389),\n",
       " ('defect', 0.7286218404769897),\n",
       " ('creed', 0.7250339984893799)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('means-end', 0.7441396713256836),\n",
       " ('typology', 0.734366774559021),\n",
       " ('causation', 0.7301516532897949),\n",
       " ('formulation', 0.7151468992233276),\n",
       " ('conceptualization', 0.7118353247642517),\n",
       " ('paradigm', 0.7072738409042358),\n",
       " ('substantive', 0.7031725645065308),\n",
       " ('systematization', 0.6971014738082886),\n",
       " ('causality', 0.693217933177948),\n",
       " ('clarification', 0.6926217675209045)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scheme', 0.7933559417724609),\n",
       " ('formulation', 0.7889211773872375),\n",
       " ('paradigm', 0.7764719128608704),\n",
       " ('typology', 0.7751098871231079),\n",
       " ('conceptualization', 0.7513638734817505),\n",
       " ('characterization', 0.7298194766044617),\n",
       " ('reformulation', 0.7205566763877869),\n",
       " ('epistemology', 0.7142667770385742),\n",
       " ('logic', 0.70758455991745),\n",
       " ('framework', 0.7057795524597168)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scheme', 0.8517293930053711),\n",
       " ('typology', 0.7872000932693481),\n",
       " ('framework', 0.7809886336326599),\n",
       " ('model', 0.7502861022949219),\n",
       " ('conceptualization', 0.7485644817352295),\n",
       " ('conception', 0.72993004322052),\n",
       " ('concept', 0.7214322090148926),\n",
       " ('formulation', 0.7199011445045471),\n",
       " ('definition', 0.7133165001869202),\n",
       " ('logic', 0.7117828130722046)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('schema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simmel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Small', 0.8588085174560547),\n",
       " ('Durkheim', 0.839136004447937),\n",
       " ('Wundt', 0.83685302734375),\n",
       " ('Sumner', 0.8282672166824341),\n",
       " ('Ross', 0.8259565830230713),\n",
       " ('Frazer', 0.8251640796661377),\n",
       " ('Tarde', 0.8244202136993408),\n",
       " ('Ellwood', 0.8210508823394775),\n",
       " ('Loria', 0.8199872970581055),\n",
       " ('Schmoller', 0.8184661865234375)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Durkheim', 0.87535160779953),\n",
       " ('Karl', 0.868350088596344),\n",
       " ('Herbert', 0.8593208193778992),\n",
       " ('Sumner', 0.8556383848190308),\n",
       " ('Mead', 0.8555867671966553),\n",
       " ('Becker', 0.8554558753967285),\n",
       " ('Spencer', 0.8531584143638611),\n",
       " ('Marx', 0.8514550924301147),\n",
       " ('Lundberg', 0.8499455451965332),\n",
       " ('Pareto', 0.847312331199646)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spencer', 0.8490694761276245),\n",
       " ('Mannheim', 0.8403051495552063),\n",
       " ('Freud', 0.8245453834533691),\n",
       " ('Mead', 0.8197968602180481),\n",
       " ('Malinowski', 0.818902850151062),\n",
       " ('Comte', 0.8144658803939819),\n",
       " ('Hegel', 0.8132918477058411),\n",
       " ('Pareto', 0.8084408640861511),\n",
       " ('Weber', 0.8053643703460693),\n",
       " ('Durkheim', 0.791892409324646)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Durkheim', 0.8430885076522827),\n",
       " ('Weber', 0.8373894691467285),\n",
       " ('Elias', 0.807611346244812),\n",
       " ('Castoriadis', 0.7955990433692932),\n",
       " ('Parsons', 0.7842784523963928),\n",
       " ('Whitehead', 0.7732088565826416),\n",
       " ('Mannheim', 0.7559753060340881),\n",
       " ('Nietzsche', 0.7544924020767212),\n",
       " ('Bergson', 0.7537936568260193),\n",
       " ('Mead', 0.7514632940292358)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.wv.most_similar('Simmel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## End of script\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## notes and draft\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TSNE vis https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "* TSNE vis http://lvdmaaten.github.io/tsne/\n",
    "* Second Tensorboard visualization demo: https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8\n",
    "* Word to Vec explained: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* Using .loc: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
    "* Gensim Word2Vec: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "* SNA in Jupyter Notebook: http://bl.ocks.org/brinrosenthal/raw/fd7d7277ce74c2b762d3a4d66326215c/\n",
    "* Same but than the blog version: http://compbio.ucsd.edu/bringing-interactivity-network-visualization-jupyter-notebooks-visjs2jupyter/\n",
    "* Nice word embedding blog\n",
    "* Presentation Manning: https://nlp.stanford.edu/manning/talks/Simons-Institute-Manning-2017.pdf\n",
    "* what about stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After the model is trained, it is accessible via the “wv” attribute. This is the actual word vector model in which queries can be made.\n",
    "\n",
    "# For example, you can print the learned vocabulary of tokens (words) as follows:\n",
    "\n",
    "words = list(model_test.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can review the embedded vector for a specific token as follows:\n",
    "\n",
    "print(model_test['system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test.wv.most_similar(positive=['system', 'social'], negative=['socialism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test.wv.doesnt_match(\"system connected developed science\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test.wv.similarity('causal', 'relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When getting started, you can save the learned model in ASCII format and review the contents.\n",
    "\n",
    "# You can do this by setting binary=False when calling the save_word2vec_format() function, for example:\n",
    "\n",
    "# model_test.wv.save_word2vec_format('model_test.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The saved model can then be loaded again by calling the Word2Vec.load() function. For example:\n",
    "# model = Word2Vec.load('model_test.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Example codes from tutorial\n",
    "#### https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# specify the main corpus path. This will be used throughout the script\n",
    "MACHINE_LEARNING = \"C:/Users/renswilderom/Documents/Machine learning\"\n",
    "\n",
    "#Specify working directory\n",
    "os.chdir(MACHINE_LEARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['normal', 'child'], negative=['school'], topn=1)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
